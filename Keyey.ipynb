{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9b7347a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57913874",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b015e2e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Laptop\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Laptop\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d29f5922",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('papers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dac8219d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7241, 7)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ceb584ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "# snippet for custom stop words\n",
    "custom_stop_words = [\n",
    "    'fig','figure','sample','image','using',\n",
    "    'show','result','large',\n",
    "    'also','one', 'two','three',\n",
    "    'four','five','six','seven','eight','nine','ten'\n",
    "]\n",
    "custom_stop_words = set(custom_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5c85314",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stop_words = list(stop_words | custom_stop_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f72eb10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(text):\n",
    "    \n",
    "    # lowercase\n",
    "    text=text.lower()\n",
    "    \n",
    "    #remove tags\n",
    "    text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n",
    "    \n",
    "    # remove special characters and digits\n",
    "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    \n",
    "    ##Convert to list from string\n",
    "    text = text.split()\n",
    "    \n",
    "    # remove stopwords\n",
    "    text = [word for word in text if word not in stop_words]\n",
    "\n",
    "    # remove words less than three letters\n",
    "    text = [word for word in text if len(word) >= 3]\n",
    "\n",
    "    # lemmatize\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    text = [lmtzr.lemmatize(word) for word in text]\n",
    "    \n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "686607b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Laptop\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0e125f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = data['paper_text'].iloc[:1000].apply(lambda x : pre_process(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28fe1ce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      self organization associative database applica...\n",
       "1      mean field theory layer visual cortex applicat...\n",
       "2      storing covariance associative long term poten...\n",
       "3      bayesian query construction neural network mod...\n",
       "4      neural network ensemble cross validation activ...\n",
       "                             ...                        \n",
       "995    farotimi demho kailath neural network weight m...\n",
       "996    foundation circuit complexity theory sensory p...\n",
       "997    reinforcement learning function approximation ...\n",
       "998    smart vision chip fabricated dimensional integ...\n",
       "999    shape context new descriptor shape matching ob...\n",
       "Name: paper_text, Length: 1000, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21e6f4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using TF-IDF\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#docs = docs.tolist()\n",
    "#create a vocabulary of words, \n",
    "cv=CountVectorizer(max_df=0.85,         # ignore words that appear in 85% of documents\n",
    "                   max_features=1500,  # the size of the vocabulary\n",
    "                   ngram_range=(1,3)    # vocabulary contains single words, bigrams, trigrams\n",
    "                  )\n",
    "cv.fit(docs)\n",
    "word_count_vector= cv.transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ac7a6c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfTransformer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfTransformer</label><div class=\"sk-toggleable__content\"><pre>TfidfTransformer()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TfidfTransformer()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(word_count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dde3c952",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d0bca6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(cv, open('countvectorizer.pkl','wb'))\n",
    "pickle.dump(tfidf_transformer,open('transformer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52adb102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_coo(coo_matrix):\n",
    "    \n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "848818fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "    \n",
    "    #use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    "\n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "\n",
    "    for idx, score in sorted_items:\n",
    "        fname = feature_names[idx]\n",
    "        \n",
    "        #keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    "\n",
    "    #create a tuples of feature,score\n",
    "    #results = zip(feature_vals,score_vals)\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1cde0810",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = cv.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9b83dd66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['computer science', 'computing', 'concept', 'conclusion'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names[230:234]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3183fd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(feature_names,open('feature_names.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0103d3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords(idx, docs):\n",
    "\n",
    "    #generate tf-idf for the given document\n",
    "    tf_idf_vector=tfidf_transformer.transform(cv.transform([docs[idx]]))\n",
    "\n",
    "    #sort the tf-idf vectors by descending order of scores\n",
    "    sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "\n",
    "    #extract only the top n; n here is 10\n",
    "    keywords=extract_topn_from_vector(feature_names,sorted_items,10)\n",
    "    \n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "12872b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords_text(docs):\n",
    "\n",
    "    #generate tf-idf for the given document\n",
    "    tf_idf_vector=tfidf_transformer.transform(cv.transform([docs]))\n",
    "\n",
    "    #sort the tf-idf vectors by descending order of scores\n",
    "    sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "\n",
    "    #extract only the top n; n here is 10\n",
    "    keywords=extract_topn_from_vector(feature_names,sorted_items,10)\n",
    "    \n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "14f043b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = docs[227]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6bac17ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = get_keywords_text(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7357095f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state 0.474\n",
      "log 0.353\n",
      "log likelihood 0.292\n",
      "circuit 0.29\n",
      "word 0.246\n",
      "likelihood 0.244\n",
      "chip 0.184\n",
      "hmm 0.132\n",
      "analog 0.132\n",
      "transistor 0.126\n"
     ]
    }
   ],
   "source": [
    "for k in keywords:\n",
    "    print(k,keywords[k])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "09c063c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(idx,keywords, df):\n",
    "    # now print the results\n",
    "    print(\"Title\")\n",
    "    print(df['title'][idx])\n",
    "    print(\"\\nAbstract\")\n",
    "    print(df['abstract'][idx])\n",
    "    print(\"\\nKeywords for this text\")\n",
    "    for k in keywords:\n",
    "        print(k,keywords[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b31e1209",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title\n",
      "Stock Selection via Nonlinear Multi-Factor Models\n",
      "\n",
      "Abstract\n",
      "Abstract Missing\n",
      "\n",
      "Keywords for this text\n",
      "return 0.605\n",
      "factor 0.386\n",
      "model 0.338\n",
      "risk 0.17\n",
      "nonlinear 0.156\n",
      "short 0.149\n",
      "long 0.123\n",
      "performance 0.119\n",
      "network 0.117\n",
      "linear model 0.105\n"
     ]
    }
   ],
   "source": [
    "idx=127\n",
    "keywords=get_keywords(idx, docs)\n",
    "print_results(idx,keywords, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded404e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
